{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c965285f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import math, random, sys\n",
    "import numpy as np\n",
    "import argparse\n",
    "from argparse import Namespace\n",
    "from collections import deque\n",
    "import pickle as pickle\n",
    "\n",
    "from jtnn import *\n",
    "from auxiliaries import build_parser, set_random_seed\n",
    "import rdkit\n",
    "import pandas as pd\n",
    "import json, os\n",
    "from rdkit import RDLogger\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f31e0f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "lg = RDLogger.logger() \n",
    "lg.setLevel(RDLogger.CRITICAL)\n",
    "root = str(pathlib.Path().absolute())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daafb5b4",
   "metadata": {},
   "source": [
    "## Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bfd8ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overwrite default parameters\n",
    "cmd_args = {\n",
    "    'beta': 0.002, \n",
    "    'max_beta': 1.0,\n",
    "    'latent_size': 4,\n",
    "}\n",
    "\n",
    "train_path = os.path.join(root, 'rafa-processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ae1c8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = os.path.join(root, 'gen_models')\n",
    "json_path = os.path.join(model_dir, 'default_gen_args.json')\n",
    "with open(json_path) as handle:\n",
    "    arguments = json.loads(handle.read())\n",
    "arguments.update(cmd_args)\n",
    "if 'seed' in arguments:\n",
    "    set_random_seed(args['seed'])\n",
    "else:\n",
    "    arguments['seed'] = set_random_seed()\n",
    "arguments['cuda'] = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130f7b8d",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a36ec14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(parametrization):\n",
    "    global arguments, model_dir\n",
    "    args = {**arguments, **parametrization}\n",
    "    model_name = f\"gen-ls{args['latent_size']}-lr{args['lr']:.3f}-h{args['hidden_size']}-l{args['num_layers']}-e{args['epoch']}-s{args['seed']}\"\n",
    "    args['save_dir'] = os.path.join(model_dir, model_name)\n",
    "    # save model settings\n",
    "    os.makedirs(args['save_dir'], exist_ok=True)\n",
    "    dump_json_path = os.path.join(args['save_dir'], 'model.json')\n",
    "    if not os.path.exists(dump_json_path):\n",
    "        with open(dump_json_path, \"w\") as fp:\n",
    "            json.dump(args, fp, sort_keys=True, indent=4)\n",
    "    args = Namespace(**args)\n",
    "    vocab = Vocab([x.strip(\"\\r\\n \") for x in open(args.vocab)])\n",
    "    print(args)\n",
    "    model = JTNNVAE(vocab, args)\n",
    "    if args.cuda:\n",
    "        model = model.cuda()\n",
    "    return trainer(model, args, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51844de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, args, vocab):\n",
    "    global train_path\n",
    "    for param in model.parameters():\n",
    "        if param.dim() == 1:\n",
    "            nn.init.constant_(param, 0)\n",
    "        else:\n",
    "            nn.init.xavier_normal_(param)\n",
    "    print((\"Model #Params: %dK\" % (sum([x.nelement() for x in model.parameters()]) / 1000,)))\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "    scheduler = lr_scheduler.ExponentialLR(optimizer, args.anneal_rate)\n",
    "    # scheduler.step()\n",
    "\n",
    "    param_norm = lambda m: math.sqrt(sum([p.norm().item() ** 2 for p in m.parameters()]))\n",
    "    grad_norm = lambda m: math.sqrt(sum([p.grad.norm().item() ** 2 for p in m.parameters() if p.grad is not None]))\n",
    "\n",
    "    total_step = args.load_epoch\n",
    "    beta = args.beta\n",
    "    meters = np.zeros(4)\n",
    "    \n",
    "    for epoch in range(args.epoch):\n",
    "        print(f\"Currently at epoch: {epoch+1}\")\n",
    "        loader = MolTreeFolder(train_path, vocab, args.batch_size, num_workers=4)\n",
    "        for batch in loader:\n",
    "            total_step += 1\n",
    "            model.zero_grad()\n",
    "            loss, kl_div, wacc, tacc, sacc = model(batch, beta)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), args.clip_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "            meters = meters + np.array([kl_div, wacc * 100, tacc * 100, sacc * 100])\n",
    "\n",
    "            if total_step % args.print_iter == 0:\n",
    "                meters /= args.print_iter\n",
    "                print((\"[%d] Beta: %.3f, KL: %.2f, Word: %.2f, Topo: %.2f, Assm: %.2f, PNorm: %.2f, GNorm: %.2f\" % (total_step, beta, meters[0], meters[1], meters[2], meters[3], param_norm(model), grad_norm(model))))\n",
    "                sys.stdout.flush()\n",
    "                meters *= 0\n",
    "\n",
    "            if total_step % args.save_iter == 0:\n",
    "                torch.save(model.state_dict(), args.save_dir + \"/model.iter-\" + str(total_step))\n",
    "\n",
    "            if total_step % args.anneal_iter == 0:\n",
    "                scheduler.step()\n",
    "                print((\"learning rate: %.6f\" % scheduler.get_lr()[0]))\n",
    "\n",
    "            if total_step % args.kl_anneal_iter == 0 and total_step >= args.warmup:\n",
    "                beta = min(args.max_beta, beta + args.step_beta)\n",
    "    \n",
    "    torch.save(model.state_dict(), args.save_dir + f\"/model\")\n",
    "    \n",
    "    return evaluate(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e61d704b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "    data = []\n",
    "    for i in range(10000):\n",
    "        data.append(model.sample_prior())\n",
    "    # df = pd.read_csv('/home/huang651/port-to-botorch/rafa-pred-model/data/rafa/mols_rafadb.csv')\n",
    "    data = list(set(data))\n",
    "    # return len(data) - smiles.isin(data).sum()\n",
    "    return len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44453ced",
   "metadata": {},
   "source": [
    "## Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76ba3eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments['epoch'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9384eef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 05-27 01:43:45] ax.modelbridge.dispatch_utils: Using Bayesian Optimization generation strategy: GenerationStrategy(name='Sobol+GPEI', steps=[Sobol for 5 trials, GPEI for subsequent trials]). Iterations after 5 will take longer to generate due to  model-fitting.\n",
      "[INFO 05-27 01:43:45] ax.service.managed_loop: Started full optimization with 20 steps.\n",
      "[INFO 05-27 01:43:45] ax.service.managed_loop: Running optimization trial 1...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(anneal_iter=40000, anneal_rate=0.9, batch_size=32, beta=0.002, clip_norm=50.0, cuda=True, depthG=3, depthT=20, epoch=25, hidden_size=243, kl_anneal_iter=2000, latent_size=4, load_epoch=0, lr=0.003665708072781563, max_beta=1.0, n_out=1, num_layers=2, print_iter=100, save_dir='/home/huang651/junction-tree/gen_models/gen2d-lr0.00367-h243-l2-e25-s3765065388', save_iter=5000, seed=3765065388, target='homo', total_trials=50, use_activation=True, vocab='/home/huang651/port-to-botorch/rafa-pred-model/data/rafa/vocab.txt', warmup=20000)\n",
      "Model #Params: 1376K\n",
      "Currently at epoch: 1\n",
      "[100] Beta: 0.002, KL: 48.87, Word: 64.62, Topo: 91.03, Assm: 93.71, PNorm: 83.41, GNorm: 24.12\n",
      "[200] Beta: 0.002, KL: 46.50, Word: 79.43, Topo: 97.54, Assm: 96.81, PNorm: 88.99, GNorm: 12.67\n",
      "[300] Beta: 0.002, KL: 46.16, Word: 81.64, Topo: 98.51, Assm: 96.98, PNorm: 93.02, GNorm: 14.99\n",
      "[400] Beta: 0.002, KL: 51.77, Word: 83.39, Topo: 98.70, Assm: 97.14, PNorm: 96.55, GNorm: 11.36\n",
      "Currently at epoch: 2\n",
      "[500] Beta: 0.002, KL: 56.88, Word: 84.05, Topo: 98.74, Assm: 96.88, PNorm: 99.93, GNorm: 15.29\n",
      "[600] Beta: 0.002, KL: 61.15, Word: 85.17, Topo: 98.79, Assm: 97.28, PNorm: 103.16, GNorm: 11.94\n",
      "[700] Beta: 0.002, KL: 61.58, Word: 85.52, Topo: 98.69, Assm: 97.13, PNorm: 106.44, GNorm: 11.27\n",
      "[800] Beta: 0.002, KL: 63.65, Word: 85.99, Topo: 98.87, Assm: 97.01, PNorm: 109.44, GNorm: 8.12\n",
      "Currently at epoch: 3\n",
      "[900] Beta: 0.002, KL: 67.65, Word: 86.57, Topo: 99.02, Assm: 97.16, PNorm: 112.28, GNorm: 10.00\n",
      "[1000] Beta: 0.002, KL: 66.86, Word: 87.04, Topo: 99.02, Assm: 97.24, PNorm: 114.72, GNorm: 9.12\n",
      "[1100] Beta: 0.002, KL: 69.05, Word: 87.00, Topo: 99.01, Assm: 97.35, PNorm: 117.92, GNorm: 12.62\n",
      "[1200] Beta: 0.002, KL: 71.99, Word: 87.06, Topo: 99.02, Assm: 96.86, PNorm: 122.16, GNorm: 8.70\n",
      "Currently at epoch: 4\n",
      "[1300] Beta: 0.002, KL: 66.31, Word: 87.18, Topo: 98.72, Assm: 97.27, PNorm: 126.33, GNorm: 8.74\n",
      "[1400] Beta: 0.002, KL: 69.93, Word: 87.81, Topo: 99.13, Assm: 96.89, PNorm: 128.47, GNorm: 8.21\n",
      "[1500] Beta: 0.002, KL: 67.58, Word: 87.67, Topo: 99.13, Assm: 97.42, PNorm: 130.49, GNorm: 8.99\n",
      "[1600] Beta: 0.002, KL: 67.46, Word: 87.91, Topo: 99.18, Assm: 97.07, PNorm: 132.64, GNorm: 6.01\n",
      "Currently at epoch: 5\n",
      "[1700] Beta: 0.002, KL: 87.82, Word: 87.85, Topo: 99.20, Assm: 96.71, PNorm: 136.05, GNorm: 8.97\n",
      "[1800] Beta: 0.002, KL: 69.21, Word: 87.64, Topo: 99.18, Assm: 96.42, PNorm: 147.56, GNorm: 17.68\n",
      "[1900] Beta: 0.002, KL: 72.00, Word: 87.82, Topo: 99.18, Assm: 97.31, PNorm: 150.23, GNorm: 50.00\n",
      "[2000] Beta: 0.002, KL: 67.43, Word: 88.35, Topo: 99.20, Assm: 97.06, PNorm: 152.18, GNorm: 10.48\n",
      "Currently at epoch: 6\n",
      "[2100] Beta: 0.002, KL: 70.31, Word: 88.17, Topo: 99.18, Assm: 97.04, PNorm: 154.10, GNorm: 13.48\n",
      "[2200] Beta: 0.002, KL: 75.27, Word: 88.52, Topo: 99.13, Assm: 96.79, PNorm: 156.97, GNorm: 13.98\n",
      "[2300] Beta: 0.002, KL: 73.73, Word: 88.16, Topo: 99.19, Assm: 97.26, PNorm: 159.19, GNorm: 8.22\n",
      "[2400] Beta: 0.002, KL: 68.42, Word: 88.55, Topo: 99.10, Assm: 97.09, PNorm: 161.69, GNorm: 9.80\n",
      "Currently at epoch: 7\n",
      "[2500] Beta: 0.002, KL: 74.68, Word: 88.82, Topo: 99.20, Assm: 97.06, PNorm: 163.31, GNorm: 10.97\n",
      "[2600] Beta: 0.002, KL: 68.63, Word: 88.74, Topo: 99.21, Assm: 97.18, PNorm: 164.85, GNorm: 12.77\n",
      "[2700] Beta: 0.002, KL: 71.48, Word: 88.44, Topo: 99.16, Assm: 97.15, PNorm: 166.55, GNorm: 12.20\n",
      "[2800] Beta: 0.002, KL: 71.20, Word: 88.73, Topo: 99.29, Assm: 96.92, PNorm: 169.16, GNorm: 22.20\n",
      "Currently at epoch: 8\n",
      "[2900] Beta: 0.002, KL: 69.96, Word: 88.47, Topo: 99.21, Assm: 96.68, PNorm: 178.93, GNorm: 12.85\n",
      "[3000] Beta: 0.002, KL: 64.13, Word: 88.70, Topo: 99.12, Assm: 97.21, PNorm: 181.11, GNorm: 9.98\n",
      "[3100] Beta: 0.002, KL: 64.58, Word: 88.89, Topo: 99.27, Assm: 97.12, PNorm: 182.28, GNorm: 7.54\n",
      "[3200] Beta: 0.002, KL: 64.40, Word: 89.27, Topo: 99.30, Assm: 97.08, PNorm: 183.50, GNorm: 7.43\n",
      "Currently at epoch: 9\n",
      "[3300] Beta: 0.002, KL: 64.36, Word: 89.15, Topo: 99.27, Assm: 97.28, PNorm: 184.80, GNorm: 9.29\n",
      "[3400] Beta: 0.002, KL: 63.73, Word: 89.35, Topo: 99.28, Assm: 97.24, PNorm: 186.44, GNorm: 5.88\n",
      "[3500] Beta: 0.002, KL: 65.30, Word: 89.21, Topo: 99.24, Assm: 97.10, PNorm: 188.17, GNorm: 8.69\n",
      "[3600] Beta: 0.002, KL: 69.62, Word: 88.59, Topo: 99.26, Assm: 96.93, PNorm: 195.93, GNorm: 10.45\n",
      "Currently at epoch: 10\n",
      "[3700] Beta: 0.002, KL: 67.43, Word: 89.30, Topo: 99.26, Assm: 97.15, PNorm: 197.37, GNorm: 9.30\n"
     ]
    }
   ],
   "source": [
    "from ax.service.managed_loop import optimize\n",
    "best_parameters, values, experiment, model = optimize(\n",
    "    parameters = [\n",
    "        { \"name\": \"lr\", \"type\": \"range\", \"bounds\": [1e-5, 5e-3] },\n",
    "        { \"name\": \"hidden_size\", \"type\": \"range\", \"value_type\": \"int\", \"bounds\": [32, 256] },\n",
    "        { \"name\": \"num_layers\", \"type\": \"range\", \"value_type\": \"int\", \"bounds\": [1, 3] },\n",
    "        # { \"name\": \"latent_size\", \"type\": \"range\", \"value_type\": \"int\", \"bounds\": [40, 100] },\n",
    "        { \"name\": \"epoch\", \"type\": \"range\", \"value_type\": \"int\", \"bounds\": [20, 60] },\n",
    "    ],\n",
    "    evaluation_function=train,\n",
    "    minimize=False,\n",
    "    total_trials=20\n",
    ")\n",
    "means, covariances = values\n",
    "print('best parameters:', best_parameters)\n",
    "print(means)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
