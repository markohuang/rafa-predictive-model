{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c965285f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import math, random, sys\n",
    "import numpy as np\n",
    "import argparse\n",
    "from argparse import Namespace\n",
    "from collections import deque\n",
    "import pickle as pickle\n",
    "\n",
    "from jtnn import *\n",
    "from auxiliaries import build_parser, set_random_seed\n",
    "import rdkit\n",
    "import pandas as pd\n",
    "import json, os\n",
    "from rdkit import RDLogger\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f31e0f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "lg = RDLogger.logger() \n",
    "lg.setLevel(RDLogger.CRITICAL)\n",
    "root = str(pathlib.Path().absolute())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daafb5b4",
   "metadata": {},
   "source": [
    "## Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bfd8ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overwrite default parameters\n",
    "cmd_args = {\n",
    "    'beta': 0.002, \n",
    "    'max_beta': 1.0,\n",
    "    'latent_size': 4,\n",
    "}\n",
    "\n",
    "train_path = os.path.join(root, 'oled-all-processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ae1c8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = os.path.join(root, 'gen_models')\n",
    "json_path = os.path.join(model_dir, 'default_gen_args.json')\n",
    "with open(json_path) as handle:\n",
    "    arguments = json.loads(handle.read())\n",
    "arguments.update(cmd_args)\n",
    "if 'seed' in arguments:\n",
    "    set_random_seed(arguments['seed'])\n",
    "else:\n",
    "    arguments['seed'] = set_random_seed()\n",
    "arguments['cuda'] = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130f7b8d",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a36ec14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(parametrization):\n",
    "    global arguments, model_dir\n",
    "    args = {**arguments, **parametrization}\n",
    "    model_name = f\"gen-ls{args['latent_size']}-lr{args['lr']:.3f}-h{args['hidden_size']}-l{args['num_layers']}-e{args['epoch']}-s{args['seed']}\"\n",
    "    args['save_dir'] = os.path.join(model_dir, model_name)\n",
    "    # save model settings\n",
    "    os.makedirs(args['save_dir'], exist_ok=True)\n",
    "    dump_json_path = os.path.join(args['save_dir'], 'model.json')\n",
    "    if not os.path.exists(dump_json_path):\n",
    "        with open(dump_json_path, \"w\") as fp:\n",
    "            json.dump(args, fp, sort_keys=True, indent=4)\n",
    "    args = Namespace(**args)\n",
    "    vocab = Vocab([x.strip(\"\\r\\n \") for x in open(args.vocab)])\n",
    "    print(args)\n",
    "    model = JTNNVAE(vocab, args)\n",
    "    if args.cuda:\n",
    "        model = model.cuda()\n",
    "    return trainer(model, args, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51844de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, args, vocab):\n",
    "    global train_path\n",
    "    for param in model.parameters():\n",
    "        if param.dim() == 1:\n",
    "            nn.init.constant_(param, 0)\n",
    "        else:\n",
    "            nn.init.xavier_normal_(param)\n",
    "    print((\"Model #Params: %dK\" % (sum([x.nelement() for x in model.parameters()]) / 1000,)))\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "    scheduler = lr_scheduler.ExponentialLR(optimizer, args.anneal_rate)\n",
    "    # scheduler.step()\n",
    "\n",
    "    param_norm = lambda m: math.sqrt(sum([p.norm().item() ** 2 for p in m.parameters()]))\n",
    "    grad_norm = lambda m: math.sqrt(sum([p.grad.norm().item() ** 2 for p in m.parameters() if p.grad is not None]))\n",
    "\n",
    "    total_step = args.load_epoch\n",
    "    beta = args.beta\n",
    "    meters = np.zeros(4)\n",
    "    \n",
    "    for epoch in range(args.epoch):\n",
    "        print(f\"Currently at epoch: {epoch+1}\")\n",
    "        loader = MolTreeFolder(train_path, vocab, args.batch_size, num_workers=4)\n",
    "        for batch in loader:\n",
    "            total_step += 1\n",
    "            model.zero_grad()\n",
    "            loss, kl_div, wacc, tacc, sacc = model(batch, beta)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), args.clip_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "            meters = meters + np.array([kl_div, wacc * 100, tacc * 100, sacc * 100])\n",
    "\n",
    "            if total_step % args.print_iter == 0:\n",
    "                meters /= args.print_iter\n",
    "                print((\"[%d] Beta: %.3f, KL: %.2f, Word: %.2f, Topo: %.2f, Assm: %.2f, PNorm: %.2f, GNorm: %.2f\" % (total_step, beta, meters[0], meters[1], meters[2], meters[3], param_norm(model), grad_norm(model))))\n",
    "                sys.stdout.flush()\n",
    "                meters *= 0\n",
    "\n",
    "            if total_step % args.save_iter == 0:\n",
    "                torch.save(model.state_dict(), args.save_dir + \"/model.iter-\" + str(total_step))\n",
    "\n",
    "            if total_step % args.anneal_iter == 0:\n",
    "                scheduler.step()\n",
    "                print((\"learning rate: %.6f\" % scheduler.get_lr()[0]))\n",
    "\n",
    "            if total_step % args.kl_anneal_iter == 0 and total_step >= args.warmup:\n",
    "                beta = min(args.max_beta, beta + args.step_beta)\n",
    "    \n",
    "    torch.save(model.state_dict(), args.save_dir + f\"/model\")\n",
    "    \n",
    "    return evaluate(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e61d704b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "    data = []\n",
    "    for i in range(10000):\n",
    "        data.append(model.sample_prior())\n",
    "    # df = pd.read_csv('/home/huang651/port-to-botorch/rafa-pred-model/data/rafa/mols_rafadb.csv')\n",
    "    data = list(set(data))\n",
    "    # return len(data) - smiles.isin(data).sum()\n",
    "    return len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44453ced",
   "metadata": {},
   "source": [
    "## Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76ba3eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments['epoch'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9384eef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 06-14 21:06:57] ax.modelbridge.dispatch_utils: Using Bayesian Optimization generation strategy: GenerationStrategy(name='Sobol+GPEI', steps=[Sobol for 5 trials, GPEI for subsequent trials]). Iterations after 5 will take longer to generate due to  model-fitting.\n",
      "[INFO 06-14 21:06:57] ax.service.managed_loop: Started full optimization with 20 steps.\n",
      "[INFO 06-14 21:06:57] ax.service.managed_loop: Running optimization trial 1...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(anneal_iter=40000, anneal_rate=0.9, batch_size=32, beta=0.002, clip_norm=50.0, cuda=True, depthG=3, depthT=20, epoch=49, hidden_size=133, kl_anneal_iter=2000, latent_size=4, load_epoch=0, lr=0.00302590354859829, max_beta=1.0, n_out=1, num_layers=3, print_iter=100, save_dir='/home/huang651/junction-tree/gen_models/gen-ls4-lr0.003-h133-l3-e49-s602289165', save_iter=5000, seed=602289165, target='homo', total_trials=50, use_activation=True, vocab='./data/rafa/vocab.txt', warmup=20000)\n",
      "Model #Params: 481K\n",
      "Currently at epoch: 1\n",
      "[100] Beta: 0.002, KL: 112.62, Word: 55.29, Topo: 85.58, Assm: 61.27, PNorm: 63.19, GNorm: 16.79\n",
      "[200] Beta: 0.002, KL: 72.77, Word: 68.40, Topo: 93.20, Assm: 70.10, PNorm: 69.51, GNorm: 20.10\n",
      "[300] Beta: 0.002, KL: 63.19, Word: 71.85, Topo: 94.09, Assm: 73.87, PNorm: 74.44, GNorm: 21.44\n",
      "[400] Beta: 0.002, KL: 56.98, Word: 73.85, Topo: 94.66, Assm: 75.43, PNorm: 78.27, GNorm: 29.28\n",
      "[500] Beta: 0.002, KL: 54.59, Word: 74.99, Topo: 94.90, Assm: 75.32, PNorm: 81.65, GNorm: 25.21\n",
      "[600] Beta: 0.002, KL: 50.47, Word: 75.16, Topo: 94.98, Assm: 76.21, PNorm: 84.67, GNorm: 15.49\n",
      "[700] Beta: 0.002, KL: 49.32, Word: 76.09, Topo: 95.23, Assm: 76.26, PNorm: 87.44, GNorm: 20.54\n",
      "[800] Beta: 0.002, KL: 46.89, Word: 76.13, Topo: 95.34, Assm: 76.91, PNorm: 89.90, GNorm: 13.81\n",
      "[900] Beta: 0.002, KL: 47.20, Word: 76.06, Topo: 95.16, Assm: 77.52, PNorm: 92.47, GNorm: 17.97\n",
      "[1000] Beta: 0.002, KL: 47.53, Word: 76.53, Topo: 95.63, Assm: 77.52, PNorm: 94.76, GNorm: 17.84\n",
      "[1100] Beta: 0.002, KL: 46.64, Word: 77.47, Topo: 95.66, Assm: 77.92, PNorm: 96.88, GNorm: 13.33\n",
      "[1200] Beta: 0.002, KL: 46.01, Word: 77.93, Topo: 95.76, Assm: 78.01, PNorm: 99.11, GNorm: 13.91\n",
      "[1300] Beta: 0.002, KL: 46.87, Word: 77.95, Topo: 95.68, Assm: 78.23, PNorm: 101.65, GNorm: 15.40\n",
      "[1400] Beta: 0.002, KL: 46.75, Word: 77.82, Topo: 95.88, Assm: 81.15, PNorm: 104.00, GNorm: 12.99\n",
      "[1500] Beta: 0.002, KL: 53.04, Word: 80.89, Topo: 96.08, Assm: 81.12, PNorm: 105.97, GNorm: 15.52\n",
      "[1600] Beta: 0.002, KL: 52.17, Word: 82.49, Topo: 96.43, Assm: 81.77, PNorm: 107.60, GNorm: 23.03\n",
      "[1700] Beta: 0.002, KL: 51.74, Word: 82.76, Topo: 96.52, Assm: 82.29, PNorm: 109.28, GNorm: 16.51\n",
      "[1800] Beta: 0.002, KL: 50.48, Word: 83.33, Topo: 96.69, Assm: 82.77, PNorm: 110.87, GNorm: 13.44\n",
      "[1900] Beta: 0.002, KL: 52.10, Word: 83.54, Topo: 96.73, Assm: 82.60, PNorm: 112.44, GNorm: 15.83\n",
      "[2000] Beta: 0.002, KL: 50.55, Word: 83.56, Topo: 96.89, Assm: 83.74, PNorm: 114.02, GNorm: 12.70\n",
      "[2100] Beta: 0.002, KL: 50.56, Word: 84.13, Topo: 96.94, Assm: 83.09, PNorm: 115.61, GNorm: 15.24\n",
      "[2200] Beta: 0.002, KL: 50.87, Word: 84.26, Topo: 97.03, Assm: 82.68, PNorm: 117.12, GNorm: 16.44\n",
      "[2300] Beta: 0.002, KL: 49.14, Word: 84.17, Topo: 96.99, Assm: 83.96, PNorm: 118.61, GNorm: 11.61\n",
      "[2400] Beta: 0.002, KL: 47.40, Word: 84.36, Topo: 97.19, Assm: 83.51, PNorm: 120.06, GNorm: 14.11\n",
      "[2500] Beta: 0.002, KL: 47.67, Word: 84.49, Topo: 97.11, Assm: 84.09, PNorm: 121.47, GNorm: 23.19\n",
      "[2600] Beta: 0.002, KL: 48.08, Word: 84.64, Topo: 97.08, Assm: 84.08, PNorm: 122.92, GNorm: 10.99\n",
      "[2700] Beta: 0.002, KL: 49.06, Word: 84.99, Topo: 97.27, Assm: 83.49, PNorm: 124.22, GNorm: 10.59\n",
      "[2800] Beta: 0.002, KL: 49.19, Word: 85.01, Topo: 97.34, Assm: 83.74, PNorm: 125.53, GNorm: 10.19\n",
      "[2900] Beta: 0.002, KL: 46.04, Word: 83.50, Topo: 93.99, Assm: 82.51, PNorm: 127.52, GNorm: 14.06\n",
      "[3000] Beta: 0.002, KL: 33.99, Word: 84.29, Topo: 89.67, Assm: 90.07, PNorm: 130.16, GNorm: 10.43\n",
      "[3100] Beta: 0.002, KL: 37.23, Word: 85.72, Topo: 91.25, Assm: 92.52, PNorm: 131.64, GNorm: 10.77\n",
      "[3200] Beta: 0.002, KL: 38.69, Word: 85.91, Topo: 91.99, Assm: 93.08, PNorm: 133.06, GNorm: 11.52\n",
      "[3300] Beta: 0.002, KL: 39.23, Word: 86.63, Topo: 92.15, Assm: 93.73, PNorm: 134.42, GNorm: 12.34\n",
      "[3400] Beta: 0.002, KL: 40.90, Word: 87.10, Topo: 92.43, Assm: 94.09, PNorm: 135.56, GNorm: 10.50\n",
      "[3500] Beta: 0.002, KL: 41.36, Word: 87.09, Topo: 92.92, Assm: 94.13, PNorm: 136.68, GNorm: 19.51\n",
      "[3600] Beta: 0.002, KL: 42.69, Word: 87.18, Topo: 93.22, Assm: 94.05, PNorm: 137.76, GNorm: 14.53\n",
      "[3700] Beta: 0.002, KL: 41.91, Word: 87.48, Topo: 93.19, Assm: 94.07, PNorm: 138.84, GNorm: 13.73\n",
      "[3800] Beta: 0.002, KL: 43.76, Word: 87.84, Topo: 93.62, Assm: 94.03, PNorm: 139.90, GNorm: 15.65\n",
      "[3900] Beta: 0.002, KL: 44.19, Word: 87.94, Topo: 93.48, Assm: 94.49, PNorm: 140.96, GNorm: 12.71\n",
      "[4000] Beta: 0.002, KL: 44.67, Word: 87.90, Topo: 93.81, Assm: 94.81, PNorm: 142.00, GNorm: 13.82\n",
      "[4100] Beta: 0.002, KL: 44.00, Word: 88.02, Topo: 93.86, Assm: 94.70, PNorm: 143.05, GNorm: 17.00\n"
     ]
    }
   ],
   "source": [
    "from ax.service.managed_loop import optimize\n",
    "best_parameters, values, experiment, model = optimize(\n",
    "    parameters = [\n",
    "        { \"name\": \"lr\", \"type\": \"range\", \"bounds\": [1e-5, 5e-3] },\n",
    "        { \"name\": \"hidden_size\", \"type\": \"range\", \"value_type\": \"int\", \"bounds\": [32, 256] },\n",
    "        { \"name\": \"num_layers\", \"type\": \"range\", \"value_type\": \"int\", \"bounds\": [1, 3] },\n",
    "        # { \"name\": \"latent_size\", \"type\": \"range\", \"value_type\": \"int\", \"bounds\": [40, 100] },\n",
    "        { \"name\": \"epoch\", \"type\": \"range\", \"value_type\": \"int\", \"bounds\": [20, 60] },\n",
    "    ],\n",
    "    evaluation_function=train,\n",
    "    minimize=False,\n",
    "    total_trials=20\n",
    ")\n",
    "means, covariances = values\n",
    "print('best parameters:', best_parameters)\n",
    "print(means)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc582cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
