{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0921989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import math, random, sys\n",
    "import numpy as np\n",
    "import argparse\n",
    "from argparse import Namespace\n",
    "from collections import deque\n",
    "import pickle as pickle\n",
    "\n",
    "from jtnn import *\n",
    "from auxiliaries import build_parser, set_random_seed, get_args_and_vocab\n",
    "import rdkit\n",
    "import json, os\n",
    "from rdkit import RDLogger\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c09d0e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "lg = RDLogger.logger() \n",
    "lg.setLevel(RDLogger.CRITICAL)\n",
    "root = str(pathlib.Path().absolute())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ca9083",
   "metadata": {},
   "source": [
    "## Arguments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "256555ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overwrite default parameters\n",
    "cmd_args = {\n",
    "    'beta': 0.002, \n",
    "    'max_beta': 1.0,\n",
    "    'latent_size': 4,\n",
    "    'target': 'splitting',\n",
    "}\n",
    "\n",
    "train_path = os.path.join(root, 'pred_processed', cmd_args['target'], 'train')\n",
    "val_path = os.path.join(root, 'pred_processed', cmd_args['target'], 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb52abe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anneal_iter': 1000,\n",
       " 'anneal_rate': 0.9,\n",
       " 'batch_size': 32,\n",
       " 'beta': 0.002,\n",
       " 'clip_norm': 50.0,\n",
       " 'cuda': True,\n",
       " 'depthG': 3,\n",
       " 'depthT': 20,\n",
       " 'epoch': 50,\n",
       " 'hidden_size': 400,\n",
       " 'kl_anneal_iter': 2000,\n",
       " 'latent_size': 4,\n",
       " 'load_epoch': 0,\n",
       " 'lr': 0.0003,\n",
       " 'max_beta': 1.0,\n",
       " 'n_out': 1,\n",
       " 'num_layers': 5,\n",
       " 'plot': True,\n",
       " 'print_iter': 100,\n",
       " 'save_iter': 100000,\n",
       " 'seed': 2595467008,\n",
       " 'step_beta': 0.002,\n",
       " 'total_trials': 20,\n",
       " 'use_activation': True,\n",
       " 'vocab': './data/rafa/vocab.txt',\n",
       " 'target': 'strength'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir = os.path.join(root, 'pred_models')\n",
    "json_path = os.path.join(model_dir, 'default_pred_args.json')\n",
    "with open(json_path) as handle:\n",
    "    arguments = json.loads(handle.read())\n",
    "arguments.update(cmd_args)\n",
    "if 'seed' in arguments:\n",
    "    set_random_seed(arguments['seed'])\n",
    "else:\n",
    "    arguments['seed'] = set_random_seed()\n",
    "arguments['cuda'] = torch.cuda.is_available()\n",
    "arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5666b79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(parametrization):\n",
    "    global arguments, model_dir\n",
    "    args = {**arguments, **parametrization}\n",
    "    model_name = \"{}-h{}-l{}-n{}-e{}-s{}\".format(\n",
    "        args['target'], args['hidden_size'], args['latent_size'],\n",
    "        args['num_layers'], args['epoch'], args['seed']\n",
    "    )\n",
    "    args['save_dir'] = os.path.join(model_dir, model_name)\n",
    "    # save model settings\n",
    "    os.makedirs(args['save_dir'], exist_ok=True)\n",
    "    dump_json_path = os.path.join(args['save_dir'], 'model.json')\n",
    "    if not os.path.exists(dump_json_path):\n",
    "        with open(dump_json_path, \"w\") as fp:\n",
    "            json.dump(args, fp, sort_keys=True, indent=4)\n",
    "    args = Namespace(**args)\n",
    "    vocab = Vocab([x.strip(\"\\r\\n \") for x in open(args.vocab)])\n",
    "    print(args)\n",
    "    model = RAFAVAE(vocab, args, evaluate=False)\n",
    "    if args.cuda:\n",
    "        model = model.cuda()\n",
    "    return trainer(model, args, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89c2bbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, args, vocab):\n",
    "    global train_path, val_path\n",
    "    for param in model.parameters():\n",
    "        if param.dim() == 1:\n",
    "            nn.init.constant_(param, 0)\n",
    "        else:\n",
    "            nn.init.xavier_normal_(param)\n",
    "\n",
    "    if args.load_epoch > 0:\n",
    "        model.load_state_dict(torch.load(args.save_dir + \"/model.iter-\" + str(args.load_epoch)))\n",
    "\n",
    "    print((\"Model #Params: %dK\" % (sum([x.nelement() for x in model.parameters()]) / 1000,)))\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "    scheduler = lr_scheduler.ExponentialLR(optimizer, args.anneal_rate, verbose=True)\n",
    "    # As per warning\n",
    "    # scheduler.step()\n",
    "\n",
    "    param_norm = lambda m: math.sqrt(sum([p.norm().item() ** 2 for p in m.parameters()]))\n",
    "    grad_norm = lambda m: math.sqrt(sum([p.grad.norm().item() ** 2 for p in m.parameters() if p.grad is not None]))\n",
    "\n",
    "    total_step = args.load_epoch\n",
    "    meters = np.zeros(1)\n",
    "\n",
    "    for epoch in range(args.epoch):\n",
    "        loader = MolTreeFolder(train_path, vocab, args.batch_size, num_workers=4)\n",
    "        val_loader = MolTreeFolder(val_path, vocab, args.batch_size, num_workers=4)\n",
    "        model.train()\n",
    "        for batch in loader:\n",
    "            total_step += 1\n",
    "            model.zero_grad()\n",
    "            loss = model(batch)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), args.clip_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "            meters = meters + np.array([loss.data.cpu().numpy()])\n",
    "\n",
    "            if total_step % args.print_iter == 0:\n",
    "                meters /= args.print_iter\n",
    "                print((\"[%d] Loss: %.2f, PNorm: %.2f, GNorm: %.2f\" % (total_step, meters[0], param_norm(model), grad_norm(model))))\n",
    "                sys.stdout.flush()\n",
    "                meters *= 0\n",
    "\n",
    "            if total_step % args.save_iter == 0:\n",
    "                torch.save(model.state_dict(), args.save_dir + \"/model.iter-\" + str(total_step))\n",
    "\n",
    "            if total_step % args.anneal_iter == 0:\n",
    "                scheduler.step()\n",
    "                print((\"learning rate: %.6f\" % scheduler.get_last_lr()[0]))\n",
    "\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        model.eval()\n",
    "        for val_batch in val_loader:\n",
    "            val_steps += 1\n",
    "            loss = model(val_batch)\n",
    "            val_loss += float(loss.data)\n",
    "\n",
    "        print((\"Validation Loss: %.6f\" % (val_loss / val_steps)))\n",
    "        sys.stdout.flush()\n",
    "    \n",
    "    torch.save(model.state_dict(), args.save_dir + f\"/model\")\n",
    "    plot_train(model, args)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "272b15e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train(model, args):\n",
    "    vocab = Vocab([x.strip(\"\\r\\n \") for x in open(args.vocab)])\n",
    "    model.set_mode(evaluate=True)\n",
    "    model.eval()\n",
    "    loader = MolTreeFolder(train_path, vocab, args.batch_size, num_workers=4)\n",
    "    pred = np.zeros(0)\n",
    "    act = np.zeros(0)\n",
    "    for batch in loader:\n",
    "        output = model(batch)\n",
    "        output = output.data.cpu().numpy().squeeze()\n",
    "        labels = np.array([x.label for x in batch[0]])\n",
    "        pred = np.concatenate((pred, output))\n",
    "        act = np.concatenate((act, labels))\n",
    "    save_dir = args.save_dir\n",
    "    np.save(os.path.join(save_dir, 'pred-train'), pred)\n",
    "    np.save(os.path.join(save_dir, 'act-train'), act)\n",
    "    import matplotlib\n",
    "    matplotlib.use('Agg')\n",
    "    import matplotlib.pyplot as plt\n",
    "    pred, act = np.load(os.path.join(save_dir, 'pred-train.npy')), np.load(os.path.join(save_dir, 'act-train.npy'))\n",
    "    plt.figure()\n",
    "    ax = plt.gca()\n",
    "    ax.plot([0,1],[0,1], transform=ax.transAxes, color='green')\n",
    "    ax.set_aspect('equal')\n",
    "    plt.scatter(act, pred, s=1)\n",
    "    plt.xlabel('Actual')\n",
    "    plt.ylabel('Predicted')\n",
    "    plt.title(args.target)\n",
    "    plt.savefig(os.path.join(save_dir, args.target+'-train.png'), dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6070c0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate(parametrization):\n",
    "    model = train(parametrization)\n",
    "    return evaluate(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5ac28a",
   "metadata": {},
   "source": [
    "## Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593078dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 06-14 21:31:29] ax.modelbridge.dispatch_utils: Using Bayesian Optimization generation strategy: GenerationStrategy(name='Sobol+GPEI', steps=[Sobol for 5 trials, GPEI for subsequent trials]). Iterations after 5 will take longer to generate due to  model-fitting.\n",
      "[INFO 06-14 21:31:29] ax.service.managed_loop: Started full optimization with 20 steps.\n",
      "[INFO 06-14 21:31:29] ax.service.managed_loop: Running optimization trial 1...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(anneal_iter=1000, anneal_rate=0.9, batch_size=32, beta=0.002, clip_norm=50.0, cuda=True, depthG=3, depthT=20, epoch=68, hidden_size=336, kl_anneal_iter=2000, latent_size=65, load_epoch=0, lr=0.004179916833043099, max_beta=1.0, n_out=1, num_layers=3, plot=True, print_iter=100, save_dir='/home/huang651/junction-tree/pred_models/strength-h336-l65-n3-e68-s2595467008', save_iter=100000, seed=2595467008, step_beta=0.002, target='strength', total_trials=20, use_activation=True, vocab='./data/rafa/vocab.txt')\n",
      "Model #Params: 2799K\n",
      "Adjusting learning rate of group 0 to 4.1799e-03.\n",
      "[100] Loss: 0.44, PNorm: 92.78, GNorm: 0.60\n",
      "[200] Loss: 0.22, PNorm: 99.67, GNorm: 0.96\n",
      "[300] Loss: 0.21, PNorm: 110.07, GNorm: 1.70\n",
      "[400] Loss: 0.20, PNorm: 114.75, GNorm: 0.21\n",
      "[500] Loss: 0.18, PNorm: 116.87, GNorm: 0.26\n",
      "[600] Loss: 0.17, PNorm: 118.54, GNorm: 0.86\n",
      "[700] Loss: 0.18, PNorm: 121.07, GNorm: 0.52\n",
      "[800] Loss: 0.18, PNorm: 128.54, GNorm: 1.12\n",
      "[900] Loss: 0.18, PNorm: 133.79, GNorm: 0.31\n",
      "[1000] Loss: 0.16, PNorm: 135.90, GNorm: 1.07\n",
      "Adjusting learning rate of group 0 to 3.7619e-03.\n",
      "learning rate: 0.003762\n",
      "[1100] Loss: 0.16, PNorm: 137.94, GNorm: 0.31\n",
      "[1200] Loss: 0.15, PNorm: 140.14, GNorm: 0.46\n",
      "[1300] Loss: 0.15, PNorm: 142.05, GNorm: 1.94\n",
      "[1400] Loss: 0.14, PNorm: 144.44, GNorm: 0.55\n",
      "[1500] Loss: 0.13, PNorm: 146.37, GNorm: 0.62\n",
      "[1600] Loss: 0.14, PNorm: 148.30, GNorm: 0.33\n",
      "[1700] Loss: 0.12, PNorm: 151.01, GNorm: 0.22\n",
      "[1800] Loss: 0.11, PNorm: 153.29, GNorm: 0.85\n",
      "[1900] Loss: 0.13, PNorm: 157.50, GNorm: 0.53\n",
      "[2000] Loss: 0.12, PNorm: 159.77, GNorm: 0.82\n",
      "Adjusting learning rate of group 0 to 3.3857e-03.\n",
      "learning rate: 0.003386\n",
      "[2100] Loss: 0.11, PNorm: 160.77, GNorm: 0.28\n",
      "[2200] Loss: 0.10, PNorm: 161.85, GNorm: 1.09\n",
      "[2300] Loss: 0.11, PNorm: 163.13, GNorm: 0.46\n",
      "[2400] Loss: 0.10, PNorm: 164.78, GNorm: 0.80\n",
      "[2500] Loss: 0.11, PNorm: 167.66, GNorm: 0.39\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter search\n",
    "from ax.service.managed_loop import optimize\n",
    "best_parameters, values, experiment, model = optimize(\n",
    "    parameters = [\n",
    "        { \"name\": \"lr\", \"type\": \"range\", \"bounds\": [1e-5, 5e-3] },\n",
    "        { \"name\": \"hidden_size\", \"type\": \"range\", \"value_type\": \"int\", \"bounds\": [100, 650] },\n",
    "        { \"name\": \"num_layers\", \"type\": \"range\", \"value_type\": \"int\", \"bounds\": [1, 5] },\n",
    "        { \"name\": \"latent_size\", \"type\": \"range\", \"value_type\": \"int\", \"bounds\": [40, 100] },\n",
    "        { \"name\": \"epoch\", \"type\": \"range\", \"value_type\": \"int\", \"bounds\": [10, 20] },\n",
    "    ],\n",
    "    evaluation_function=train_evaluate,\n",
    "    minimize=True,\n",
    "    total_trials=20\n",
    ")\n",
    "means, covariances = values\n",
    "print('best parameters:', best_parameters)\n",
    "print(means)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfee18a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
