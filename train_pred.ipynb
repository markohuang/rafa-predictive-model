{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0921989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import math, random, sys\n",
    "import numpy as np\n",
    "import argparse\n",
    "from argparse import Namespace\n",
    "from collections import deque\n",
    "import pickle as pickle\n",
    "\n",
    "from jtnn import *\n",
    "from auxiliaries import build_parser, set_random_seed, get_args_and_vocab\n",
    "import rdkit\n",
    "import json, os\n",
    "from rdkit import RDLogger\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c09d0e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "lg = RDLogger.logger() \n",
    "lg.setLevel(RDLogger.CRITICAL)\n",
    "root = str(pathlib.Path().absolute())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ca9083",
   "metadata": {},
   "source": [
    "## Arguments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256555ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overwrite default parameters\n",
    "cmd_args = {\n",
    "    'beta': 0.002, \n",
    "    'max_beta': 1.0,\n",
    "    'latent_size': 4,\n",
    "    'target': 'splitting',\n",
    "}\n",
    "\n",
    "train_path = os.path.join(root, 'pred_processed', cmd_args['target'], 'train')\n",
    "val_path = os.path.join(root, 'pred_processed', cmd_args['target'], 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb52abe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = os.path.join(root, 'pred_models')\n",
    "json_path = os.path.join(model_dir, 'default_pred_args.json')\n",
    "with open(json_path) as handle:\n",
    "    arguments = json.loads(handle.read())\n",
    "arguments.update(cmd_args)\n",
    "if 'seed' in args:\n",
    "    set_random_seed(args['seed'])\n",
    "else:\n",
    "    arguments['seed'] = set_random_seed()\n",
    "arguments['cuda'] = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5666b79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(parametrization):\n",
    "    global arguments, model_dir\n",
    "    args = {**arguments, **parametrization}\n",
    "    model_name = \"{}-h{}-l{}-n{}-e{}-s{}\".format(\n",
    "        args['target'], args['hidden_size'], args['latent_size'],\n",
    "        args['num_layers'], args['epoch'], args['seed']\n",
    "    )\n",
    "    args['save_dir'] = os.path.join(model_dir, model_name)\n",
    "    # save model settings\n",
    "    os.makedirs(args['save_dir'], exist_ok=True)\n",
    "    dump_json_path = os.path.join(args['save_dir'], 'model.json')\n",
    "    if not os.path.exists(dump_json_path):\n",
    "        with open(dump_json_path, \"w\") as fp:\n",
    "            json.dump(args, fp, sort_keys=True, indent=4)\n",
    "    args = Namespace(**args)\n",
    "    vocab = Vocab([x.strip(\"\\r\\n \") for x in open(args.vocab)])\n",
    "    print(args)\n",
    "    model = RAFAVAE(vocab, args, evaluate=False)\n",
    "    if args.cuda:\n",
    "        model = model.cuda()\n",
    "    return trainer(model, args, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89c2bbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, args, vocab):\n",
    "    global train_path, val_path\n",
    "    for param in model.parameters():\n",
    "        if param.dim() == 1:\n",
    "            nn.init.constant_(param, 0)\n",
    "        else:\n",
    "            nn.init.xavier_normal_(param)\n",
    "\n",
    "    if args.load_epoch > 0:\n",
    "        model.load_state_dict(torch.load(args.save_dir + \"/model.iter-\" + str(args.load_epoch)))\n",
    "\n",
    "    print((\"Model #Params: %dK\" % (sum([x.nelement() for x in model.parameters()]) / 1000,)))\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "    scheduler = lr_scheduler.ExponentialLR(optimizer, args.anneal_rate, verbose=True)\n",
    "    # As per warning\n",
    "    # scheduler.step()\n",
    "\n",
    "    param_norm = lambda m: math.sqrt(sum([p.norm().item() ** 2 for p in m.parameters()]))\n",
    "    grad_norm = lambda m: math.sqrt(sum([p.grad.norm().item() ** 2 for p in m.parameters() if p.grad is not None]))\n",
    "\n",
    "    total_step = args.load_epoch\n",
    "    meters = np.zeros(1)\n",
    "\n",
    "    for epoch in range(args.epoch):\n",
    "        loader = MolTreeFolder(train_path, vocab, args.batch_size, num_workers=4)\n",
    "        val_loader = MolTreeFolder(val_path, vocab, args.batch_size, num_workers=4)\n",
    "        model.train()\n",
    "        for batch in loader:\n",
    "            total_step += 1\n",
    "            model.zero_grad()\n",
    "            loss = model(batch)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), args.clip_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "            meters = meters + np.array([loss.data.cpu().numpy()])\n",
    "\n",
    "            if total_step % args.print_iter == 0:\n",
    "                meters /= args.print_iter\n",
    "                print((\"[%d] Loss: %.2f, PNorm: %.2f, GNorm: %.2f\" % (total_step, meters[0], param_norm(model), grad_norm(model))))\n",
    "                sys.stdout.flush()\n",
    "                meters *= 0\n",
    "\n",
    "            if total_step % args.save_iter == 0:\n",
    "                torch.save(model.state_dict(), args.save_dir + \"/model.iter-\" + str(total_step))\n",
    "\n",
    "            if total_step % args.anneal_iter == 0:\n",
    "                scheduler.step()\n",
    "                print((\"learning rate: %.6f\" % scheduler.get_last_lr()[0]))\n",
    "\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        model.eval()\n",
    "        for val_batch in val_loader:\n",
    "            val_steps += 1\n",
    "            loss = model(val_batch)\n",
    "            val_loss += float(loss.data)\n",
    "\n",
    "        print((\"Validation Loss: %.6f\" % (val_loss / val_steps)))\n",
    "        sys.stdout.flush()\n",
    "    \n",
    "    torch.save(model.state_dict(), args.save_dir + f\"/model\")\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "272b15e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train(model, best_parameters):\n",
    "    global arguments, train_path\n",
    "    args = {**arguments, **best_parameters}\n",
    "    args = Namespace(**args)\n",
    "    vocab = Vocab([x.strip(\"\\r\\n \") for x in open(args.vocab)])\n",
    "    model.set_mode(evaluate=True)\n",
    "    model.eval()\n",
    "    loader = MolTreeFolder(train_path, vocab, args.batch_size, num_workers=4)\n",
    "    pred = np.zeros(0)\n",
    "    act = np.zeros(0)\n",
    "    for batch in loader:\n",
    "        output = model(batch)\n",
    "        output = output.data.cpu().numpy().squeeze()\n",
    "        labels = np.array([x.label for x in batch[0]])\n",
    "        pred = np.concatenate((pred, output))\n",
    "        act = np.concatenate((act, labels))\n",
    "    save_dir = args.save_dir\n",
    "    np.save(os.path.join(save_dir, 'pred-train'), pred)\n",
    "    np.save(os.path.join(save_dir, 'act-train'), act)\n",
    "    import matplotlib\n",
    "    matplotlib.use('Agg')\n",
    "    import matplotlib.pyplot as plt\n",
    "    pred, act = np.load(os.path.join(save_dir, 'pred-train.npy')), np.load(os.path.join(save_dir, 'act-train.npy'))\n",
    "    plt.figure()\n",
    "    ax = plt.gca()\n",
    "    ax.plot([0,1],[0,1], transform=ax.transAxes, color='green')\n",
    "    ax.set_aspect('equal')\n",
    "    plt.scatter(act, pred, s=1)\n",
    "    plt.xlabel('Actual')\n",
    "    plt.ylabel('Predicted')\n",
    "    plt.title(args.target)\n",
    "    plt.savefig(os.path.join(save_dir, args.target+'-train.png'), dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6070c0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate(parametrization):\n",
    "    model = train(parametrization)\n",
    "    return evaluate(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5ac28a",
   "metadata": {},
   "source": [
    "## Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593078dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter search\n",
    "from ax.service.managed_loop import optimize\n",
    "best_parameters, values, experiment, model = optimize(\n",
    "    parameters = [\n",
    "        { \"name\": \"lr\", \"type\": \"range\", \"bounds\": [1e-5, 5e-3] },\n",
    "        { \"name\": \"hidden_size\", \"type\": \"range\", \"value_type\": \"int\", \"bounds\": [100, 650] },\n",
    "        { \"name\": \"num_layers\", \"type\": \"range\", \"value_type\": \"int\", \"bounds\": [1, 5] },\n",
    "        { \"name\": \"latent_size\", \"type\": \"range\", \"value_type\": \"int\", \"bounds\": [40, 100] },\n",
    "        { \"name\": \"epoch\", \"type\": \"range\", \"value_type\": \"int\", \"bounds\": [20, 100] },\n",
    "    ],\n",
    "    evaluation_function=train_evaluate,\n",
    "    minimize=True,\n",
    "    total_trials=args.total_trials\n",
    ")\n",
    "means, covariances = values\n",
    "print('best parameters:', best_parameters)\n",
    "print(means)\n",
    "\n",
    "# re-train best model\n",
    "best_parameters['plot'] = True\n",
    "model = train(best_parameters)\n",
    "torch.save(model.state_dict(), os.path.join(args.save_dir, f\"/{args.target}-model\"))\n",
    "best_model_param = { **vars(args), **best_parameters }\n",
    "with open(dump_json_path, \"w\") as fp:\n",
    "    json.dump(best_model_param, fp, sort_keys=True, indent=4)\n",
    "\n",
    "# plot pred v. act for train set\n",
    "plot_train(model)\n",
    "\n",
    "# log best_parameters and objective separately\n",
    "# ilogpath = os.path.join(args.save_dir, 'model.json')\n",
    "# with open(ilogpath, 'w') as f:\n",
    "#     log = 'Best parameters:\\n'\n",
    "#     log += '\\n'.join(f'{u:<11}: {v}' for u, v in best_parameters.items())\n",
    "#     log += f\"\\n\\nObjective: {means['objective']}\"\n",
    "#     f.write(log)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
